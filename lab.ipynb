{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABORATORIO #2 - Deteccion de SPAM\n",
    " Alfredo Quezada 191002\n",
    " \n",
    " Randy Venegas   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Exploracion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>3031</td>\n",
       "      <td>empty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>3012</td>\n",
       "      <td>Once upon a time, Jesse wrote :&gt; Have you thou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>5080</td>\n",
       "      <td>\"Adam L. Beberg\"  writes:&gt; &gt; Interestingly, it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>4339</td>\n",
       "      <td>\"Leonard R. Cleavelin\" wrote:\\n&gt;Scat I have, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>5561</td>\n",
       "      <td>Another thing I see in debian but not in my RH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               Body  Label\n",
       "3031        3031                                              empty      0\n",
       "3012        3012  Once upon a time, Jesse wrote :> Have you thou...      0\n",
       "5080        5080  \"Adam L. Beberg\"  writes:> > Interestingly, it...      0\n",
       "4339        4339  \"Leonard R. Cleavelin\" wrote:\\n>Scat I have, u...      0\n",
       "5561        5561  Another thing I see in debian but not in my RH...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agregamos el data set para poder tener una idea de lo que son los datos. \n",
    "import pandas as pd \n",
    "df1 = pd.read_csv('completeSpamAssassin.csv')\n",
    "df1.sample(n=5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9036</th>\n",
       "      <td>20810</td>\n",
       "      <td>20810</td>\n",
       "      <td>Subject: si server will be unavailable , today...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7068</th>\n",
       "      <td>31180</td>\n",
       "      <td>31180</td>\n",
       "      <td>Subject: fw : a weygandt , andrew ; glover , r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5588</th>\n",
       "      <td>28134</td>\n",
       "      <td>28134</td>\n",
       "      <td>Subject: re :\\n shirley ,\\n i will take half d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>5801</td>\n",
       "      <td>5801</td>\n",
       "      <td>Subject: xanax for sale , no prior prescriptio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>9236</td>\n",
       "      <td>9236</td>\n",
       "      <td>Subject: tatts award claims\\n tatts lotto\\n in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  \\\n",
       "9036         20810       20810   \n",
       "7068         31180       31180   \n",
       "5588         28134       28134   \n",
       "870           5801        5801   \n",
       "4276          9236        9236   \n",
       "\n",
       "                                                   Body  Label  \n",
       "9036  Subject: si server will be unavailable , today...      0  \n",
       "7068  Subject: fw : a weygandt , andrew ; glover , r...      0  \n",
       "5588  Subject: re :\\n shirley ,\\n i will take half d...      0  \n",
       "870   Subject: xanax for sale , no prior prescriptio...      1  \n",
       "4276  Subject: tatts award claims\\n tatts lotto\\n in...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargamos tambien el segundo conjunto de datos.\n",
    "df2 = pd.read_csv('enronSpamSubset.csv')\n",
    "df2.sample(n=5).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya sabemos lo que son los dos conjuntos de datasets, procedemos a unificar los dos para crear un data set que contenga ambas caracteristicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label\n",
       "0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...      1\n",
       "1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "3  ##############################################...      1\n",
       "4  I thought you might like these:\\n1) Slim Down ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora hacemos el data set final \n",
    "#Como se puede ver eliminados los datos que no son relevantes\n",
    "df1.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df2.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df2.drop(['Unnamed: 0.1'], axis=1, inplace=True)\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.head()\n",
    "#Por ultimo guardamos el nuevo data set en un csv final.\n",
    "df.to_csv('dbfinal.csv', index=False)\n",
    "db = pd.read_csv('dbfinal.csv')\n",
    "db.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta parte realizada, ahora podemos pasar a la parte de preprocesamiento\n",
    "\n",
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nsave up to 70% on life insurance.\\nwhy spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1) fight the risk of cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1) fight the risk of cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i thought you might like these:\\n1) slim down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label\n",
       "0  \\nsave up to 70% on life insurance.\\nwhy spend...      1\n",
       "1  1) fight the risk of cancer!\\nhttp://www.adcli...      1\n",
       "2  1) fight the risk of cancer!\\nhttp://www.adcli...      1\n",
       "3  ##############################################...      1\n",
       "4  i thought you might like these:\\n1) slim down ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo primero es elimanar todos los elementos no deseados y poner los datos en el mismo formato \n",
    "db['Body'] = db['Body'].astype(str).str.lower()\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save up to  on life insurance  why spend more...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i thought you might like these   slim down   g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label\n",
       "0   save up to  on life insurance  why spend more...      1\n",
       "1    fight the risk of cancer    slim down   guar...      1\n",
       "2    fight the risk of cancer    slim down   guar...      1\n",
       "3                                                ...      1\n",
       "4  i thought you might like these   slim down   g...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora eliminamos los formatos no deseados \n",
    "import re\n",
    "db['Body']= db['Body'].apply(lambda x: re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", x))\n",
    "db['Body']= db['Body'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save up to  on life insurance  why spend more...</td>\n",
       "      <td>1</td>\n",
       "      <td>[save, up, to, on, life, insurance, why, spend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fight, the, risk, of, cancer, slim, down, gua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fight, the, risk, of, cancer, slim, down, gua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label  \\\n",
       "0   save up to  on life insurance  why spend more...      1   \n",
       "1    fight the risk of cancer    slim down   guar...      1   \n",
       "2    fight the risk of cancer    slim down   guar...      1   \n",
       "\n",
       "                                          text_token  \n",
       "0  [save, up, to, on, life, insurance, why, spend...  \n",
       "1  [fight, the, risk, of, cancer, slim, down, gua...  \n",
       "2  [fight, the, risk, of, cancer, slim, down, gua...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#Se procede a tokenizar las palabras\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "db['text_token']=db['Body'].apply(regexp.tokenize)\n",
    "db.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Randy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save up to  on life insurance  why spend more...</td>\n",
       "      <td>1</td>\n",
       "      <td>[save, life, insurance, spend, life, quote, sa...</td>\n",
       "      <td>save life insurance spend life quote savings e...</td>\n",
       "      <td>save life insurance spend life quote savings e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fight, risk, cancer, slim, guaranteed, lose, ...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fight, risk, cancer, slim, guaranteed, lose, ...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label  \\\n",
       "0   save up to  on life insurance  why spend more...      1   \n",
       "1    fight the risk of cancer    slim down   guar...      1   \n",
       "2    fight the risk of cancer    slim down   guar...      1   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [save, life, insurance, spend, life, quote, sa...   \n",
       "1  [fight, risk, cancer, slim, guaranteed, lose, ...   \n",
       "2  [fight, risk, cancer, slim, guaranteed, lose, ...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  save life insurance spend life quote savings e...   \n",
       "1  fight risk cancer slim guaranteed lose lbs day...   \n",
       "2  fight risk cancer slim guaranteed lose lbs day...   \n",
       "\n",
       "                                         text_string  \n",
       "0  save life insurance spend life quote savings e...  \n",
       "1  fight risk cancer slim guaranteed lose lbs day...  \n",
       "2  fight risk cancer slim guaranteed lose lbs day...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Hace una lista de las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Extiende las listas con stopwords\n",
    "my_stopwords = ['https']\n",
    "stopwords.extend(my_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save up to  on life insurance  why spend more...</td>\n",
       "      <td>[save, life, insurance, spend, life, quote, sa...</td>\n",
       "      <td>save life insurance spend life quote savings e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>[fight, risk, cancer, slim, guaranteed, lose, ...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight the risk of cancer    slim down   guar...</td>\n",
       "      <td>[fight, risk, cancer, slim, guaranteed, lose, ...</td>\n",
       "      <td>fight risk cancer slim guaranteed lose lbs day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>[adult, club, offers, free, membership, instan...</td>\n",
       "      <td>adult club offers free membership instant acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i thought you might like these   slim down   g...</td>\n",
       "      <td>[thought, might, like, slim, guaranteed, lose,...</td>\n",
       "      <td>thought might like slim guaranteed lose lbs da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  \\\n",
       "0   save up to  on life insurance  why spend more...   \n",
       "1    fight the risk of cancer    slim down   guar...   \n",
       "2    fight the risk of cancer    slim down   guar...   \n",
       "3                                                ...   \n",
       "4  i thought you might like these   slim down   g...   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [save, life, insurance, spend, life, quote, sa...   \n",
       "1  [fight, risk, cancer, slim, guaranteed, lose, ...   \n",
       "2  [fight, risk, cancer, slim, guaranteed, lose, ...   \n",
       "3  [adult, club, offers, free, membership, instan...   \n",
       "4  [thought, might, like, slim, guaranteed, lose,...   \n",
       "\n",
       "                                         text_string  \n",
       "0  save life insurance spend life quote savings e...  \n",
       "1  fight risk cancer slim guaranteed lose lbs day...  \n",
       "2  fight risk cancer slim guaranteed lose lbs day...  \n",
       "3  adult club offers free membership instant acce...  \n",
       "4  thought might like slim guaranteed lose lbs da...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Quita las stopwords\n",
    "db['text_token'] = db['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "db.head(3)\n",
    "# Quita las palabras infrecuentes \n",
    "db['text_clean'] = db['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "db[['Body', 'text_token', 'text_clean']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una lista con todas las palabras\n",
    "from nltk.tokenize import *\n",
    "all_words = ' '.join([word for word in db['text_clean']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Randy/nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Randy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Tokenize \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenized_words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mtokenize\u001b[39m.\u001b[39;49mword_tokenize(all_words)\n",
      "File \u001b[1;32mc:\\users\\randy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\users\\randy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\users\\randy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\users\\randy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\users\\randy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Randy/nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\randy\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Randy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Tokenize \n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n\u001b[1;32m----> 2\u001b[0m fdist \u001b[39m=\u001b[39m FreqDist(tokenized_words)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Most common words\u001b[39;00m\n\u001b[0;32m      4\u001b[0m fdist\u001b[39m.\u001b[39mmost_common(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_words)\n",
    "# Las tres palabras mas comunes\n",
    "fdist.most_common(3)\n",
    "\n",
    "# Las tres palabras mÃ¡s incomunes\n",
    "fdist.most_common()[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quita las palabras comunes\n",
    "db['text_clean']=  db['text_clean'].str.replace(\"subject\", \" \")\n",
    "db['text_clean']=  db['text_clean'].str.replace(\"enron\", \" \")\n",
    "db['text_clean']=  db['text_clean'].str.replace(\"com\", \" \")\n",
    "db['text_clean']=  db['text_clean'].str.replace(\"email\", \" \")\n",
    "\n",
    "# Vuelve a crear una lista con todas las palabras\n",
    "all_words = ' '.join([word for word in db['text_clean']])\n",
    "# Tokenize \n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_words)\n",
    "# Vuelve a imprimir las tres mas comunes\n",
    "fdist.most_common(3)\n",
    "\n",
    "# Quita las palabras mas frecuentes \n",
    "db['text_fdist'] = db['text_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] >= 1 ]))\n",
    "db[['Body', 'text_token', 'text_clean', 'text_fdist']].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a1992a9a4a1f53c325cb8c67090677e3ee02fabffe2666a579f15fac0bf94a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
